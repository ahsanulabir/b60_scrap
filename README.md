# ğŸŒ Bangladesh News Scraper

**Automated multi-source news aggregator with AI-powered analysis**

[![GitHub Actions](https://img.shields.io/badge/GitHub-Actions-2088FF?logo=github-actions&logoColor=white)](https://github.com/features/actions)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-47A248?logo=mongodb&logoColor=white)](https://www.mongodb.com/)

## ğŸ“° Supported News Sources

| #   | Source         | Language   | Type         |
| --- | -------------- | ---------- | ------------ |
| 1   | Prothom Alo    | ğŸ‡§ğŸ‡© Bangla  | RSS          |
| 2   | The Daily Star | ğŸ‡¬ğŸ‡§ English | Web Scraping |
| 3   | TBS News       | ğŸ‡¬ğŸ‡§ English | RSS          |
| 4   | BD Pratidin    | ğŸ‡§ğŸ‡© Bangla  | RSS          |
| 5   | BBC World      | ğŸ‡¬ğŸ‡§ English | RSS          |
| 6   | Jago News 24   | ğŸ‡§ğŸ‡© Bangla  | RSS          |
| 7   | Bangla Tribune | ğŸ‡§ğŸ‡© Bangla  | RSS          |
| 8   | BD24Live       | ğŸ‡§ğŸ‡© Bangla  | RSS          |

## âœ¨ Features

- âœ… **Multi-source scraping** from 8 major news outlets
- âœ… **AI-powered analysis** using Google Gemini
- âœ… **Automatic duplicate detection**
- âœ… **Direct MongoDB integration**
- âœ… **Telegram notifications** (optional)
- âœ… **Bangla & English support**
- âœ… **Clickbait detection & correction**
- âœ… **MCQ generation** from news articles
- âœ… **Category classification**
- âœ… **Importance scoring** (1-10)
- âœ… **60-word summaries** in both languages
- âœ… **GitHub Actions** for automated scheduling

## ğŸš€ Quick Start

### Local Development

1. **Clone the repository**

   ```bash
   git clone https://github.com/yourusername/bangladesh-news-scraper.git
   cd bangladesh-news-scraper
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment**

   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

4. **Run scrapers**

   ```bash
   # Run all scrapers
   python main.py

   # Run specific scraper
   python main.py --scraper prothomalo

   # List available scrapers
   python main.py --list
   ```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file with the following variables:

```env
# MongoDB (Required)
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/database
MONGODB_DATABASE=briefly60

# Gemini API (Required - comma-separated for multiple keys)
GEMINI_API_KEYS=key1,key2,key3

# Telegram (Optional)
TELEGRAM_BOT_TOKEN=your_bot_token
TELEGRAM_CHAT_ID=your_chat_id

# Scraper Settings
MAX_ARTICLES=10
REQUEST_TIMEOUT=30
```

### GitHub Actions Setup

1. **Add GitHub Secrets**
   - Go to Settings â†’ Secrets and variables â†’ Actions
   - Add these secrets:
     - `MONGODB_URI`
     - `MONGODB_DATABASE`
     - `GEMINI_API_KEYS`
     - `TELEGRAM_BOT_TOKEN` (optional)
     - `TELEGRAM_CHAT_ID` (optional)

2. **Configure Schedule**
   - Edit `.github/workflows/scraper.yml`
   - Default: Runs every 6 hours
   - Cron format: `'0 */6 * * *'`

3. **Manual Trigger**
   - Go to Actions â†’ News Scraper Cron Job
   - Click "Run workflow"
   - Select specific scraper or run all

## ğŸ“ Project Structure

```
bangladesh-news-scraper/
â”œâ”€â”€ scrapers/              # Individual scraper modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prothomalo.py
â”‚   â”œâ”€â”€ dailystar.py
â”‚   â”œâ”€â”€ tbs.py
â”‚   â”œâ”€â”€ bdpratidin.py
â”‚   â”œâ”€â”€ bbc.py
â”‚   â”œâ”€â”€ jagonews24.py
â”‚   â”œâ”€â”€ bangla_tribune.py
â”‚   â””â”€â”€ bd24live.py
â”œâ”€â”€ utils/                 # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ helpers.py         # Helper functions
â”‚   â”œâ”€â”€ gemini_ai.py       # AI integration
â”‚   â”œâ”€â”€ database.py        # MongoDB operations
â”‚   â””â”€â”€ telegram.py        # Telegram notifications
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraper.yml    # GitHub Actions workflow
â”œâ”€â”€ main.py               # Main controller
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example         # Environment template
â””â”€â”€ README.md            # This file
```

## ğŸ¯ Usage Examples

### Run All Scrapers

```bash
python main.py
# or
python main.py --scraper all
```

### Run Specific Scraper

```bash
python main.py --scraper prothomalo
python main.py --scraper dailystar
python main.py --scraper bbc
```

### List Available Scrapers

```bash
python main.py --list
```

## ğŸ“Š Output Format

Each article is saved with the following structure:

```json
{
  "title": "Article title",
  "corrected_title": "Corrected title if clickbait",
  "source": "News source name",
  "source_url": "Article URL",
  "content": "Full article text",
  "banner": "Image URL",
  "published_at": "2024-01-20T10:00:00+06:00",
  "category": "Politics",
  "summary_60_bn": "60-word Bangla summary",
  "summary_60_en": "60-word English summary",
  "importance": 8,
  "clickbait_score": 2,
  "clickbait_reason": "Reason if clickbait detected",
  "keywords": ["keyword1", "keyword2"],
  "quiz_questions": [
    {
      "question": "Quiz question",
      "options": ["A", "B", "C", "D"],
      "correct_answer": "A"
    }
  ]
}
```

## ğŸ”„ Automation

### GitHub Actions Workflow

The scraper runs automatically via GitHub Actions:

- **Schedule**: Every 6 hours (customizable)
- **Manual trigger**: Available via Actions tab
- **Artifact storage**: Results saved for 7 days
- **Environment**: Ubuntu latest with Python 3.11

### Cron Schedule Examples

```yaml
# Every 6 hours
- cron: "0 */6 * * *"

# Every day at 9 AM UTC
- cron: "0 9 * * *"

# Every hour
- cron: "0 * * * *"

# Every 30 minutes
- cron: "*/30 * * * *"
```

## ğŸ› ï¸ Development

### Adding a New Scraper

1. Create a new file in `scrapers/` (e.g., `newsource.py`)
2. Implement the scraping function following this template:

```python
from typing import List, Dict
from utils import (
    config,
    fetch_url,
    parse_html,
    generate_summary_with_gemini,
    db_handler,
    send_to_telegram
)

def scrape_newsource() -> List[Dict]:
    """Scraper for New Source"""
    # Implementation here
    pass
```

3. Register in `scrapers/__init__.py`:

```python
from .newsource import scrape_newsource

SCRAPERS = {
    # ... existing scrapers
    'newsource': scrape_newsource,
}
```

## ğŸ“ Requirements

- Python 3.11+
- MongoDB database
- Google Gemini API keys
- Telegram Bot (optional)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- News sources for providing RSS feeds
- Google Gemini for AI analysis
- MongoDB for database services
- GitHub Actions for automation

## ğŸ“§ Contact

For issues and questions, please open an issue on GitHub.

---

**Made with â¤ï¸ for Bangladesh News Aggregation**
